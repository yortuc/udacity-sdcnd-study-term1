{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 128  # Decrease batch size if you don't have enough memory\n",
    "display_step = 1\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden_layer = 256 # layer number of features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# save the model\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "x_flat = tf.reshape(x, [-1, n_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hidden layer with RELU activation\n",
    "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']),\\\n",
    "    biases['hidden_layer'])\n",
    "layer_1 = tf.nn.relu(layer_1)\n",
    "# Output layer with linear activation\n",
    "logits = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model and save weights in a checkpoint file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch  0  cost: 160.599\n",
      "batch  100  cost: 76.4689\n",
      "batch  200  cost: 53.3405\n",
      "batch  300  cost: 41.2121\n",
      "batch  400  cost: 32.7454\n",
      "batch  0  cost: 34.2398\n",
      "batch  100  cost: 26.3816\n",
      "batch  200  cost: 24.4922\n",
      "batch  300  cost: 24.2749\n",
      "batch  400  cost: 24.5296\n",
      "batch  0  cost: 18.5309\n",
      "batch  100  cost: 24.5866\n",
      "batch  200  cost: 26.3665\n",
      "batch  300  cost: 19.7308\n",
      "batch  400  cost: 19.5298\n",
      "batch  0  cost: 18.1703\n",
      "batch  100  cost: 14.8778\n",
      "batch  200  cost: 15.1368\n",
      "batch  300  cost: 16.2207\n",
      "batch  400  cost: 23.8236\n",
      "batch  0  cost: 18.0325\n",
      "batch  100  cost: 16.7599\n",
      "batch  200  cost: 13.065\n",
      "batch  300  cost: 15.9327\n",
      "batch  400  cost: 16.7682\n",
      "batch  0  cost: 10.0289\n",
      "batch  100  cost: 12.3051\n",
      "batch  200  cost: 12.7843\n",
      "batch  300  cost: 11.8741\n",
      "batch  400  cost: 14.4594\n",
      "batch  0  cost: 10.6987\n",
      "batch  100  cost: 11.2076\n",
      "batch  200  cost: 9.76909\n",
      "batch  300  cost: 11.0351\n",
      "batch  400  cost: 8.98249\n",
      "batch  0  cost: 10.0283\n",
      "batch  100  cost: 12.4808\n",
      "batch  200  cost: 13.2352\n",
      "batch  300  cost: 13.9488\n",
      "batch  400  cost: 15.0079\n",
      "batch  0  cost: 12.1527\n",
      "batch  100  cost: 12.219\n",
      "batch  200  cost: 14.4975\n",
      "batch  300  cost: 8.88962\n",
      "batch  400  cost: 10.1996\n",
      "batch  0  cost: 9.86556\n",
      "batch  100  cost: 11.205\n",
      "batch  200  cost: 8.87592\n",
      "batch  300  cost: 9.47833\n",
      "batch  400  cost: 7.72598\n",
      "batch  0  cost: 13.2345\n",
      "batch  100  cost: 8.68282\n",
      "batch  200  cost: 13.4401\n",
      "batch  300  cost: 11.1516\n",
      "batch  400  cost: 8.12578\n",
      "batch  0  cost: 8.01008\n",
      "batch  100  cost: 7.32812\n",
      "batch  200  cost: 8.42259\n",
      "batch  300  cost: 5.40066\n",
      "batch  400  cost: 5.29545\n",
      "batch  0  cost: 4.66935\n",
      "batch  100  cost: 11.4959\n",
      "batch  200  cost: 8.61168\n",
      "batch  300  cost: 6.26864\n",
      "batch  400  cost: 11.2159\n",
      "batch  0  cost: 10.222\n",
      "batch  100  cost: 7.4236\n",
      "batch  200  cost: 8.71618\n",
      "batch  300  cost: 11.9066\n",
      "batch  400  cost: 8.38215\n",
      "batch  0  cost: 8.2621\n",
      "batch  100  cost: 10.1384\n",
      "batch  200  cost: 6.9083\n",
      "batch  300  cost: 9.99126\n",
      "batch  400  cost: 6.26466\n",
      "batch  0  cost: 7.21648\n",
      "batch  100  cost: 5.46706\n",
      "batch  200  cost: 8.88685\n",
      "batch  300  cost: 11.7149\n",
      "batch  400  cost: 9.88313\n",
      "batch  0  cost: 3.81678\n",
      "batch  100  cost: 9.76342\n",
      "batch  200  cost: 7.01924\n",
      "batch  300  cost: 4.37468\n",
      "batch  400  cost: 6.94758\n",
      "batch  0  cost: 3.75051\n",
      "batch  100  cost: 13.9773\n",
      "batch  200  cost: 7.79894\n",
      "batch  300  cost: 6.81096\n",
      "batch  400  cost: 10.466\n",
      "batch  0  cost: 7.70233\n",
      "batch  100  cost: 7.33538\n",
      "batch  200  cost: 7.37346\n",
      "batch  300  cost: 10.0711\n",
      "batch  400  cost: 5.82037\n",
      "batch  0  cost: 6.83083\n",
      "batch  100  cost: 6.5476\n",
      "batch  200  cost: 9.16487\n",
      "batch  300  cost: 7.09115\n",
      "batch  400  cost: 4.6879\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(\"batch \", i, \" cost:\" ,sess.run(cost, feed_dict={x: batch_x, y: batch_y}))\n",
    "        \n",
    "        # save variables and graph at the end of each epoch\n",
    "        saver.save(sess, 'mnist_train.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weights from checkpoint file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight:\n",
      "{'out': array([[ 0.77515125,  0.76653665,  0.23600657, ...,  1.13217282,\n",
      "         1.70375657, -0.41786399],\n",
      "       [-0.55796444, -0.56523001,  0.99760729, ..., -0.21260837,\n",
      "         1.28083146,  1.31736183],\n",
      "       [ 0.26670685, -1.66613293,  0.17105938, ...,  1.14021707,\n",
      "        -0.74041057, -1.20927918],\n",
      "       ..., \n",
      "       [-0.8750658 ,  1.06064546, -0.16145529, ..., -0.01000651,\n",
      "         0.76775277, -2.13004017],\n",
      "       [ 0.48071474,  0.2127696 , -0.55893117, ...,  0.12653527,\n",
      "         0.66182113,  1.05212891],\n",
      "       [ 0.68393266, -0.50702709,  0.2689898 , ...,  0.54449379,\n",
      "        -0.38351452, -0.0302322 ]], dtype=float32), 'hidden_layer': array([[ 0.61769837,  1.9835372 , -0.96184623, ..., -0.16253449,\n",
      "         0.07400039, -0.54214603],\n",
      "       [-1.7768873 , -0.18600348,  0.0883043 , ...,  1.13179886,\n",
      "         0.23239987, -2.17374873],\n",
      "       [-2.2662375 , -1.20192683,  0.84404689, ...,  0.92117137,\n",
      "         1.08115101,  0.38586208],\n",
      "       ..., \n",
      "       [ 0.79639149, -0.48871791, -1.75774455, ..., -0.11801269,\n",
      "        -0.41893151,  0.66004407],\n",
      "       [-1.67989993,  1.35012484,  1.09760427, ...,  1.0402174 ,\n",
      "        -1.94645059, -0.20859151],\n",
      "       [ 0.49498364,  1.10865152, -0.30434617, ..., -0.52029991,\n",
      "         0.34992355,  2.65345883]], dtype=float32)}\n",
      "Bias:\n",
      "{'out': array([ 0.33871815,  1.48031592, -1.57958901, -0.43840659,  0.11101901,\n",
      "       -0.94444555,  0.1992781 , -1.60207295, -0.88981402, -0.09865002], dtype=float32), 'hidden_layer': array([ -7.57863641e-01,  -3.13088447e-01,   7.10807443e-01,\n",
      "        -2.07362485e+00,  -3.90176952e-01,   1.39932644e+00,\n",
      "         6.19194090e-01,   1.84728909e+00,   2.17601252e+00,\n",
      "         2.22265080e-01,   6.61658272e-02,  -7.44703948e-01,\n",
      "         2.89203501e+00,  -2.22561702e-01,  -1.77640557e-01,\n",
      "         4.45954740e-01,   1.08656123e-01,  -1.32670772e+00,\n",
      "        -4.61990058e-01,   1.61888614e-01,  -2.95999169e-01,\n",
      "         1.23396778e+00,   3.19001265e-02,   2.36875042e-01,\n",
      "        -3.80940139e-01,   2.47314644e+00,  -1.81085312e+00,\n",
      "        -1.83327639e+00,  -3.69980961e-01,   1.87563717e+00,\n",
      "         7.59550110e-02,   1.39259055e-01,   1.99417460e+00,\n",
      "        -1.14238155e+00,  -7.71162987e-01,  -1.12384140e+00,\n",
      "        -3.99574488e-01,  -1.35975316e-01,  -5.60621202e-01,\n",
      "         1.79829383e+00,  -7.78976500e-01,  -5.62245309e-01,\n",
      "        -8.66496801e-01,  -9.39028025e-01,   3.03579569e-01,\n",
      "         4.72263664e-01,   1.61381090e+00,  -6.12733126e-01,\n",
      "         1.61602879e+00,  -7.48357594e-01,  -2.40691543e-01,\n",
      "         3.26820821e-01,   1.08883369e+00,  -4.78745788e-01,\n",
      "        -3.75270069e-01,   1.40033066e+00,   1.38424563e+00,\n",
      "        -3.67323339e-01,   4.86115485e-01,  -7.59991109e-01,\n",
      "         1.49867069e-02,  -1.22800492e-01,   5.26047528e-01,\n",
      "         3.75029951e-01,   9.37080681e-02,  -2.83801138e-01,\n",
      "         9.03285861e-01,   3.07305390e-03,  -1.00450873e+00,\n",
      "        -9.32977140e-01,  -9.11521018e-01,  -9.07050446e-03,\n",
      "         1.18127787e+00,   5.97589195e-01,  -5.32822490e-01,\n",
      "        -4.50311810e-01,   1.06824845e-01,   5.51667392e-01,\n",
      "         2.00487539e-01,  -5.20239711e-01,   3.55020583e-01,\n",
      "        -2.27398157e-01,  -8.47177625e-01,   8.51600289e-01,\n",
      "        -1.56024456e+00,  -1.18345284e+00,   2.99168617e-01,\n",
      "        -9.73133802e-01,  -2.26222456e-01,   9.37031507e-01,\n",
      "        -2.27776896e-02,  -1.04684103e+00,  -1.66451824e+00,\n",
      "        -1.61213744e+00,  -1.24317157e+00,  -3.40438753e-01,\n",
      "        -5.36518097e-01,   1.10366893e+00,  -7.84965381e-02,\n",
      "         6.96232319e-01,  -2.82214254e-01,  -9.21113849e-01,\n",
      "        -3.10105860e-01,  -4.09463674e-01,   1.70500487e-01,\n",
      "         1.56256124e-01,  -2.86490858e-01,  -1.16756216e-01,\n",
      "         5.92646718e-01,   2.15960324e-01,  -4.37752366e-01,\n",
      "         6.40346587e-01,   3.99885893e-01,   1.30694717e-01,\n",
      "         1.56708920e+00,   1.25750303e+00,  -6.44684732e-01,\n",
      "        -1.05787468e+00,  -3.32396269e-01,  -1.49599147e+00,\n",
      "         3.58953655e-01,   8.02693963e-01,   1.05847025e+00,\n",
      "        -4.62065250e-01,  -8.89553368e-01,   5.26813805e-01,\n",
      "        -4.75988835e-01,  -9.68884945e-01,  -4.17436749e-01,\n",
      "         6.69335246e-01,   2.72163361e-01,   2.13304162e-01,\n",
      "        -1.24624097e+00,   1.49245238e+00,   4.09198627e-02,\n",
      "        -1.40368521e+00,   1.92857635e+00,  -9.39190388e-01,\n",
      "         2.14805269e+00,  -1.13379204e+00,   8.26180279e-01,\n",
      "         6.76045597e-01,  -5.99702299e-01,   4.37865794e-01,\n",
      "        -7.41706908e-01,  -2.11176351e-01,   5.86472511e-01,\n",
      "        -9.13805440e-02,   1.21673036e+00,   4.45759296e-01,\n",
      "         2.47273713e-01,   1.05062556e+00,  -1.46263552e+00,\n",
      "         2.10359603e-01,   7.64915228e-01,   5.67783937e-02,\n",
      "        -2.64467299e-01,  -2.46598780e-01,  -6.50373697e-01,\n",
      "        -1.17927885e+00,   7.30761170e-01,  -2.83123225e-01,\n",
      "        -5.22803783e-01,  -2.08633050e-01,  -1.45503879e+00,\n",
      "         3.94190401e-01,   2.51063496e-01,  -2.28831261e-01,\n",
      "        -1.09079885e+00,   1.28074539e+00,  -5.43807447e-01,\n",
      "         1.38555300e+00,  -8.81550968e-01,  -5.36113381e-01,\n",
      "        -3.34263802e-01,  -6.44038677e-01,  -4.57668692e-01,\n",
      "        -6.26842797e-01,   5.48881948e-01,  -1.24898970e+00,\n",
      "        -1.26042998e+00,   5.00651300e-01,   7.75752604e-01,\n",
      "        -4.32466418e-01,  -5.19345045e-01,  -3.96736383e-01,\n",
      "         2.23080158e+00,   2.61247098e-01,  -1.01451194e+00,\n",
      "         1.90065849e+00,  -1.43895924e+00,  -3.26711893e-01,\n",
      "        -1.23860288e+00,  -1.96512714e-01,   1.76832199e+00,\n",
      "         2.88325816e-01,  -6.05005145e-01,   2.05246043e+00,\n",
      "         2.16873661e-01,  -1.96194756e+00,   1.15615046e+00,\n",
      "        -8.22535753e-01,  -1.86414552e+00,   1.64418191e-01,\n",
      "        -1.39536485e-01,   5.01708925e-01,  -1.36048138e+00,\n",
      "        -7.13768780e-01,   9.65250075e-01,  -7.01880038e-01,\n",
      "         7.21878409e-01,   1.91878414e+00,   1.78411472e+00,\n",
      "        -1.35360646e+00,  -4.62729126e-01,   4.87276524e-01,\n",
      "        -9.48502302e-01,  -2.38698289e-01,   1.64585936e+00,\n",
      "         3.07085454e-01,   2.03412682e-01,  -1.29100531e-01,\n",
      "         1.83133215e-01,  -1.49056196e-01,  -2.55028874e-01,\n",
      "         2.31255269e+00,   2.27205467e+00,   5.38262427e-02,\n",
      "        -6.23763263e-01,  -8.01716089e-01,   1.22816956e+00,\n",
      "         5.03424585e-01,  -9.61099088e-01,   6.11367403e-04,\n",
      "         9.13670361e-01,   1.92209616e-01,   1.12507713e+00,\n",
      "        -1.96278811e+00,   1.13896906e+00,  -6.61428422e-02,\n",
      "        -7.03832746e-01,  -7.31855631e-01,  -5.40564418e-01,\n",
      "         1.55243421e+00,   9.41904008e-01,  -1.17284751e+00,\n",
      "        -3.26628923e-01,  -3.80878061e-01,  -1.53307402e+00,\n",
      "        -9.32717383e-01,  -1.48597372e+00,   8.86207223e-01,\n",
      "        -7.11830556e-01,   1.22178209e+00,  -1.15466319e-01,\n",
      "         3.71486008e-01], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# remove previous graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# re-define variables, because we removed them above\n",
    "weights = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # restore weight and biases\n",
    "    saver.restore(sess, './mnist_train.ckpt')\n",
    "    \n",
    "    # Show the values of weights and bias\n",
    "    print('Weight:')\n",
    "    print(sess.run(weights))\n",
    "    print('Bias:')\n",
    "    print(sess.run(biases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
